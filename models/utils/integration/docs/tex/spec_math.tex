This section summarizes key equations used in the implementation
of the \ModelDesc.

\subsection{Nomenclature}
\begin{tabular}{@{\hspace{0.25in}} p{1in} p{5in}}
$t_i$ &
  Dynamic time at the start of the integration cycle. \\
$t_f$ &
  Dynamic time at the end of the integration cycle. \\
$\Delta t$ &
  Dynamic time step: $\Delta t \equiv t_f - t_i$. \\
$t_n$ &
  Dynamic time at the start of intermediate step $n$. \\
$\vect x(t)$ &
  Generalized position at time $t$. \\
$\vect v(t)$ &
  Generalized velocity at time $t$. \\
$\dot {\vect x}(\vect x, \vect v)$ &
  Generalized position time derivative; provided either as an input to the
  {\tt integrate} method or calculated by a provided derivative function. \\
$\dot {\vect v}(t, \vect x(t), \vect v(t))$ &
  Generalized velocity time derivative; calculated prior to calling the
  {\tt integrate} method and provided as an input to the method. \\
$\vect s(t)$ &
  Conjoined state, comprising
  the generalized position $\vect x$ and
  the generalized velocity $\vect v$. \\
$\dot {\vect s}(t, \vect s(t))$ &
  Conjoined state time derivative, comprising
  the generalized position time derivative $\dot {\vect x}$ and
  the generalized velocity time derivative $\dot {\vect v}$.
\end{tabular}

\subsection{Euler Method}
The Euler method is the simplest of the integrators.
It conceptually propagates the conjoined state $\vect s$
by assuming that the mean value is the derivative
at the initial point of the integration interval.
The equations that govern propagation via the basic Euler method
are given by equations~(\ref{eqn:euler}).

\begin{equation}
\label{eqn:euler}
\begin{split}
t_1 &= t_f = t_i + \Delta t \\
\bar{\dot{\vect s}} &= \dot{\vect s}(t_i,\vect s(t_i)) \\
\vect s(t_f) &= \vect s(t_i) + \Delta t\ \, \bar{\dot{\vect s}}
\end{split}
\end{equation}

\subsection{Symplectic Euler Method}
The symplectic Euler method reverses the computations of the Euler method.
It propagates the generalized velocity to the end of the integration
interval and then propagates generalized position using the updated
generalized velocity.
The equations that govern propagation via the symplectic Euler method
are given by equations~(\ref{eqn:symplectic_euler}).

\begin{equation}
\label{eqn:symplectic_euler}
\begin{split}
t_1 &= t_f = t_i + \Delta t \\
\bar{\dot{\vect v}} &= \vect a(t_i,\vect x(t_i),\vect v(t_i)) \\
\vect v(t_f) &= \vect v(t_i) + \Delta t \, \bar{\dot{\vect v}} \\
\bar{\dot{\vect x}} &= \dot {\vect x}(\vect x(t_i),\vect v(t_f)) \\
\vect x(t_f) &= \vect x(t_i) + \Delta t \, \bar{\dot{\vect x}}
\end{split}
\end{equation}

\subsection{RK2 (Heun's Method)}
Heun's method is a second order two stage Runge Kutta method.
The first stage involves an Euler step to the end of the interval.
The second stage computes the mean value as the average of
the state derivative at the start of the interval and
the state derivative at the end of the interval
(as computed with the state from the first step).
Table~\ref{tab:heun_butcher} specifies the Butcher tableau for Heun's method.

\begin{table}[htp]
\centering
\caption{Heun's Method Butcher Tableau}
\label{tab:heun_butcher}
\vspace{1.5ex}
\begin{tabular}{c|cc}
0 && \\
1 & 1 & \\
\hline
& 1/2 & 1/2
\end{tabular}
\end{table}

Equations~(\ref{eqn:rk2_heun_0}) and~(\ref{eqn:rk2_heun_1}) describe
the equations that govern propagation using Heun's method.
 
Stage 0 (predictor):
\begin{equation}
\label{eqn:rk2_heun_0}
\begin{split}
t_1 &= t_f = t_i + \Delta t \\
\dot{\vect s}_0 &= \dot{\vect s}(t_i,\vect s(t_i)) \\
\vect s_1(t_1) &= \vect s(t_i) +  \Delta t\, \dot{\vect s}_0
\end{split}
\end{equation}
Stage 1 (corrector):
\begin{equation}
\label{eqn:rk2_heun_1}
\begin{split}
t_2 &= t_f = t_1 = t_i + \Delta t \\
\dot{\vect s}_1 &= \dot{\vect s}(t_1,\vect s(t_1)) \\
\bar {\dot{\vect s}} &= \frac{\dot{\vect s}_0+\dot{\vect s}_1} 2 \\ 
\vect s(t_f) &= \vect s(t) + \Delta t\, \bar {\dot{\vect s}}
\end{split}
\end{equation}

There are an infinite number of Runge Kutta methods for
any given order and number of stages.
For example, another second order two stage Runge Kutta method
is the midpoint method.
The midpoint method takes an Euler step to the midpoint of the
interval and uses the derivative at this midpoint as the mean value.

\subsection{RK4}
The fourth order, four stage Runge Kutta method is the classical fourth
order Runge Kutta method. Table~\ref{tab:rk4_butcher} specifies the
Butcher tableau for this method.
 
\begin{table}[htp]
\centering
\caption{RK4 Butcher Tableau}
\label{tab:rk4_butcher}
\vspace{1.5ex}
\begin{tabular}{c|cccc}
0 &&&& \\
1/2 & 1/2 &&& \\
1/2 & 0 & 1/2 && \\
1 & 0 & 0 & 1 & \\
\hline
& 1/6 & 1/3 & 1/3 & 1/6
\end{tabular}
\end{table}

The equations that govern propagation via the standard RK4 method are given by
equations~(\ref{eqn:rk4_0}) to~(\ref{eqn:rk4_3}).

Stage 0:
\begin{equation}
\label{eqn:rk4_0}
\begin{split}
t_1 &= t_i + \frac{\Delta t}2 \\
\dot{\vect s}_0 &= \dot{\vect s}(t_i,\vect s(t_i)) \\
\vect s_1(t_1) &= \vect s(t_i) + \frac{\Delta t}2 \, \dot{\vect s}_0
\end{split}
\end{equation}
Stage 1:
\begin{equation}
\label{eqn:rk4_1}
\begin{split}
t_2 &= t_1 = t_i + \frac{\Delta t}2 \\
\dot{\vect s}_1 &= \dot{\vect s}(t_1,\vect s_1(t_1)) \\
\vect s_2(t_2) &= \vect s(t_i) + \frac{\Delta t}2 \, \dot{\vect s}_1
\end{split}
\end{equation}
Stage 2:
\begin{equation}
\label{eqn:rk4_2}
\begin{split}
t_3 &= t_f = t_i + \Delta t \\
\dot{\vect s}_2 &= \dot{\vect s}(t_2,\vect s_1(t_2)) \\
\vect s_3(t_3) &= \vect s(t_i) + \Delta t \, \dot{\vect s}_2
\end{split}
\end{equation}
Stage 3 (final):
\begin{equation}
\label{eqn:rk4_3}
\begin{split}
\dot{\vect s}_3 &= \dot{\vect s}(t_3,\vect s_1(t_3)) \\
t_4 &= t_3 = t_f = t_i + \Delta t \\
\bar {\dot{\vect s}} &=
  \frac{\dot{\vect s}_0 + 2\dot{\vect s}_1 +
        2\dot{\vect s}_2+\dot{\vect s}_3} 6 \\
\vect s(t_f) &= \vect s(t_i) + \Delta t \, \bar {\dot{\vect s}}
\end{split}
\end{equation}

\subsection{Beeman's Algorithm}\label{sec:math_form_beeman}
All of the above approaches except for the symplectic Euler method
use the augmented state approach. The symplectic Euler method
explicitly distinguishes generalized position and generalized velocity.
Beeman's Algorithm is similar to the symplectic Euler method in this regard.
The equations that govern propagation via Beeman's Algorithm are given by
equations~(\ref{eqn:beeman_prime}) to~(\ref{eqn:beeman_1})\cite{beeman:1976}.

Priming (one simulation step):
\begin{equation}
\label{eqn:beeman_prime}
\begin{split}
\dot{\vect v}_{-1} &=
  \dot{\vect v}(t_i-\Delta t, x(t_i-\Delta t),\vect v(t_i-\Delta t)) \\
\end{split}
\end{equation}
Stage 0 (predictor):
\begin{equation}
\label{eqn:beeman_0}
\begin{split}
t_1 &= t_f = t_i + \Delta t \\
\dot{\vect v}_{0} &=
  \dot{\vect v}(t_i, x(t_i),\vect v(t_i)) \\
\bar{\vect v}_1 &= \vect v(t_i) +
   \Delta t
   \left(\frac 2 3 \dot{\vect v}_{0} - \frac 1 6 \dot{\vect v}_{-1}\right) \\
\bar{\dot{\vect x}}_1 &= \dot {\vect x}(\vect x(t_i),\bar{\vect v}_1) \\
\bar{\dot{\vect v}}_1 &=
  \frac 3 2 \dot{\vect v}_{0} - \frac 1 2 \dot{\vect v}_{-1} \\
\vect x(t_1) &= \vect x(t_i) + \Delta t \, \bar{\dot{\vect x}}_1 \\
\vect v(t_1) &= \vect v(t_i) + \Delta t \, \bar{\dot{\vect v}}_1
\end{split}
\end{equation}
Stage 1 (corrector):
\begin{equation}
\label{eqn:beeman_1}
\begin{split}
t_2 &= t_1 = t_f = t_i + \Delta t \\
\dot{\vect v}_{1} &=
  \dot{\vect v}(t_1, x(t_1),\vect v(t_1)) \\
\bar{\dot{\vect v}}_2 &=
  \frac 1 3 \dot{\vect v}_{1} +
  \frac 5 6 \dot{\vect v}_{0} - \frac 1 6 \dot{\vect v}_{-1} \\
\vect x(t_f) &= \vect x(t_1) \\
\vect v(t_f) &= \vect v(t_i) + \Delta t \, \bar{\dot{\vect v}}_2 \\
\dot{\vect v}_{-1} &= \dot{\vect v}_{0}
\end{split}
\end{equation}


Beeman's Algorithm offers significant improvements in accuracy over
the symplectic Euler method. Beeman's algorithm is a second order method
while the Euler methods are first order. Like the fourth-order
Adams-Bashforth-Moulton method, Beeman's Algorithm
is not self-starting. It relies on the derivatives from the previous
time step. JEOD uses the RK2 method as a primer for Beeman's Algorithm.


\subsection{ABM4}

Adams-Bashforth-Moulton methods combine an explicit Adams-Bashforth
integrator as a predictor and an implicit Adams-Moulton integrator
as a corrector. The fourth order Adams-Bashforth-Moulton method
implemented in JEOD uses the four step Adams-Bashforth integrator
as a predictor and the three step Adams-Moulton integrator as a corrector.

\subsubsection{Maintaining the Derivative History}
All but the simplest of Adams-Bashforth-Moulton methods require a history
of prior derivatives. JEOD uses the fourth order Adams-Bashforth-Moulton method
which requires four sets of prior derivatives.

On the first step of an integration cycle the JEOD implementation of the
ABM4 method saves the input derivatives in a derivative history buffer.
How derivatives are saved depends on whether the requisite set of four prior
derivatives have been gathered. The method operates in priming mode until the
requisite set of four prior derivatives have been gathered.

In priming mode, the derivatives are stored consecutively in the slot designated
by a primer counter which advances from zero to three as derivatives are
gathered. Once primed, the derivative history buffer is treated as a circular
array. New derivatives are copied into the slot occupied by the oldest data
and pointers are advanced circularly.

\subsubsection{Integrating State}
The state is advanced on each integration step. The primer is used to
advance the state while the method is in priming mode.
Once the requisite four sets of derivatives from the primer have been gathered,
the ABM4 integrator uses a two stage predictor-corrector technique to advance
the state.
The equations that govern propagation via the ABM4 method are given by
equations~(\ref{eqn:abm4_0}) and~(\ref{eqn:abm4_1})\cite{hamming:1986}.

Stage 0 (predictor):
\begin{equation}
\label{eqn:abm4_0}
\begin{split}
\bar {\dot{\vect s}}_{AB} &=
  \frac{55\dot{\vect s}_0 - 59\dot{\vect s}_{-1} +
        37\dot{\vect s}_{-2} - 9\dot{\vect s}_{-3}} {24} \\
t_1 &= t_f = t_i + \Delta t \\
\vect s_1(t_1) &= \vect s(t) + \Delta t \, \bar {\dot{\vect s}}_{AB}
\end{split}
\end{equation}

Stage 1 (corrector):
\begin{equation}
\label{eqn:abm4_1}
\begin{split}
\dot{\vect s}_1 &= \dot{\vect s}(t_1,\vect s(t_1)) \\
\bar {\dot{\vect s}}_{AM} &=
  \frac{9\dot{\vect s}_1 +19\dot{\vect s}_0 -
        5\dot{\vect s}_{-1} + \dot{\vect s}_{-2}} {24} \\
t_2 &= t_1 = t_f = t_i + \Delta t \\
\vect s(t_2) &= \vect s(t) + \Delta t \, \bar {\dot{\vect s}}_{AM}
\end{split}
\end{equation}

\subsubsection{ABM4 Primer}
The simplest of the Adams-Bashforth-Moulton methods combines explicit
and implicit Euler integration. This simplest implementation does not
rely on historical data. All other Adams-Bashforth-Moulton methods,
including the fourth order method implemented in JEOD,
do rely on historical data; they are not self-starting.
Some other technique must be used to prime the method.
Once primed the Adams-Bashforth-Moulton methods are self-sustaining.
As both the fourth order Adams-Bashforth-Moulton method and the fourth order
Runge Kutta method have fourth order accuracy, JEOD uses the RK4 method
as a primer for the ABM4 method.



\subsection{Gauss-Jackson Variable-Order}
Because this implementation of the integrator is a variable order, the 
coefficients are not fixed.  The derivation of those coefficients, and the mathematical details of the algorithm are extensive, and are presented in Appendix\ref{app:gj_deriv}.

After priming (including initialization of the internal values $\gamma$ and $\alpha$), the algorithm proceeds as follows:

Stage 0 (predictor)
\begin{equation}
\label{eqn:gauss_jackson_0}
\begin{split}
t_1 &= t_f = t_i + \Delta t \\
\dot{\vect v}_{0} &=
  \dot{\vect v}(t_i, x(t_i),\vect v(t_i)) \\
\alpha_0 &= \alpha_{-1} + \dot{\vect v}_{1} \\
\gamma_{0}&=\gamma_{-1}+\alpha _{-1}+ \dot{\vect v}_{0} \\
\vect{x}_P(t_1) &= \left(\Delta t\right)^{2} \left(\gamma_{0}+
 \sum_{i=0}^{N}G_{Pi}({\dot{\vect v}}_{-i})\right) \\
\vect v(t_1) &= \Delta t
   \left(\alpha_{0} + \sum_{i=0}^{N}A_{Pi}\ \dot{\vect v}_{-i}\right) \\
\end{split}
\end{equation}

Stage 1 (corrector)
\begin{equation}
\label{eqn:gauss_jackson_1}
\begin{split}
\dot{\vect v}_{1} &=
  \dot{\vect v}(t_1, x_P(t_1),\vect v(t_1)) \\
\vect{x}_C(t_1) &= \left(\Delta t\right)^{2} \left(\gamma_{0}+
 \sum_{i=0}^{N}G_{Ci}({\dot{\vect v}}_{1-i})\right) \\
\vect v(t_1) &= \Delta t
   \left(\alpha_{0} + \sum_{i=0}^{N}A_{Ci}\ \dot{\vect v}_{1-i}\right) \\
\end{split}
\end{equation}

Stage 1A (evaluation - optional)
Tests
\begin{equation*}
\abs{\frac{\vect{x}_C(t_1) - \vect{x}_P(t_1)}{\vect{x}_P(t_1)}} < \epsilon 
\end{equation*}
\ \ \ \ \ \ \ with $\epsilon$ some specified convergence criterion.  

\ \ \ \ \ \ \ if test fails:
\begin{equation*}
\vect{x}_P(t_1) = \vect{x}_C(t_1)
\end{equation*}
\ \ \ \ \ \ \ \ \ \ \ \ Return to stage 1

\ \ \ \ \ \ \ else (test succeeds) 

\ \ \ \ \ \ \ \ \ \ \ \ proceed to stage 2.

Stage 2 (convergence met)
\begin{equation}
\begin{split}
\vect{x}(t_1) &= \vect{x}_C(t_1) \\
\alpha_{-1} &= \alpha_{0} \\
\gamma_{-1} &= \gamma_0 \\
\end{split}
\end{equation}



\subsection{Technique Summary}

Table~\ref{tab:integ_technique_interfaces} presents a summary of
the integration techniques described above.

\begin{table}[htp]
\centering
\caption{Integration Technique Characteristics}
\label{tab:integ_technique_interfaces}
\vspace{1ex}
\begin{tabular}{||l|cccccc|}
\hline
{\bf Technique} &
\tilt{\bf Accuracy (order)} &
\tilt{\bf Needs Priming} &
\tilt{\bf Number Stages Per Cycle (Priming)} & 
\tilt{\bf Number Stages Per Cycle (Primed)} & 
\tilt{\bf First Step 2nd Derivatives} &
\tilt{\bf Supply 1st Derivatives} \\ \hline \hline
Euler &
  $O(\Delta t)$ & No & N/A & 1 & Yes & Yes \\
Symplectic Euler &
  $O(\Delta t)$ & No & N/A & 1 & Yes & No \\
RK2 &
  $O(\Delta t^2)$ & No & N/A & 2 & Yes & Yes \\
RK4 &
  $O(\Delta t^4)$ & No & N/A & 4 & Yes & Yes \\
Beeman's Algorithm &
  $O(\Delta t^2)$ & Yes & 2 & 2 & Yes & No \\
ABM4 &
  $O(\Delta t^4)$ & Yes & 4 & 2 & Yes & Yes \\
Gauss-Jackson &
  var. & Yes & 4 & var. & Yes & Yes \\
\hline
\end{tabular}
\end{table}

\textbf{First Step 2nd Derivative} identifies whether the second-derivative data value must be passed in to the integrator (from outside) before the first step in the integration-cycle.  Some integrators use a stored value from the end of the previous time-step, but none of those are found in the \ModelDesc.

\textbf{Supply 1st Derivative} identifies whether the time-derivative of the zeroth-derivative-state must be passed in to the integrator (from outside).  For the symplectic Euler and Beeman algorithms, the propagation of the zeroth-derivative-state (e.g. position) is based on an internally-generated mean-value of its derivative.

